{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1849853b",
   "metadata": {},
   "source": [
    "# Part-I: Theoretical Understanding\n",
    "\n",
    "## Task 1: Conceptual Questions\n",
    "\n",
    "1. **What is the difference between RNN and LSTM?**  \n",
    "RNNs process sequences step by step but struggle with long-term dependencies due to vanishing gradients. LSTMs use memory cells and gates (input, forget, output) to retain information over longer time spans.\n",
    "\n",
    "2. **What is the vanishing gradient problem, and how does LSTM solve it?**  \n",
    "In long sequences, gradients shrink during backpropagation, causing learning failure. LSTMs maintain a constant error flow using a cell state and gating mechanisms, which reduces vanishing gradients.\n",
    "\n",
    "3. **Explain the purpose of the Encoder-Decoder architecture.**  \n",
    "It maps variable-length input sequences to variable-length outputs by compressing the input into a context vector (encoder) and generating outputs step by step (decoder).\n",
    "\n",
    "4. **In a sequence-to-sequence model, what are the roles of the encoder and decoder?**  \n",
    "The encoder processes the input and produces a fixed-length representation (context vector), while the decoder generates the target sequence using this vector.\n",
    "\n",
    "5. **How is attention different from a basic encoder-decoder model?**  \n",
    "Attention allows the decoder to focus on specific encoder states at each step rather than relying on a single fixed vector, improving performance for longer inputs.\n",
    "\n",
    "## Task 2: Sequence-to-Sequence Data Flow\n",
    "\n",
    "Input sequence → Encoder (LSTM) → Hidden states → Context Vector → Decoder (LSTM) → Output sequence\n",
    "\n",
    "Labels:\n",
    "- Input: x1, x2, …, xn\n",
    "- Encoder hidden states: h1, h2, …, hn\n",
    "- Context vector: h_n\n",
    "- Decoder output: y1, y2, …, ym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fcf114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "input_texts = ['hello', 'how are you', 'good morning', 'thank you', 'i love you']\n",
    "target_texts = ['bonjour', 'comment ça va', 'bonjour', 'merci', 'je t\\'aime']\n",
    "target_texts = ['<start> ' + t + ' <end>' for t in target_texts]\n",
    "\n",
    "input_tokenizer = Tokenizer()\n",
    "input_tokenizer.fit_on_texts(input_texts)\n",
    "input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
    "input_sequences = pad_sequences(input_sequences, padding='post')\n",
    "\n",
    "output_tokenizer = Tokenizer()\n",
    "output_tokenizer.fit_on_texts(target_texts)\n",
    "output_sequences = output_tokenizer.texts_to_sequences(target_texts)\n",
    "output_sequences = pad_sequences(output_sequences, padding='post')\n",
    "\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "output_vocab_size = len(output_tokenizer.word_index) + 1\n",
    "max_encoder_len = input_sequences.shape[1]\n",
    "max_decoder_len = output_sequences.shape[1]\n",
    "\n",
    "encoder_input_data = np.array(input_sequences)\n",
    "decoder_input_data = np.array(output_sequences[:, :-1])\n",
    "decoder_target_data = np.array(output_sequences[:, 1:])\n",
    "\n",
    "latent_dim = 256\n",
    "encoder_inputs = Input(shape=(max_encoder_len,))\n",
    "enc_emb = Embedding(input_vocab_size, latent_dim)(encoder_inputs)\n",
    "encoder_lstm, state_h, state_c = LSTM(latent_dim, return_state=True)(enc_emb)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(max_decoder_len-1,))\n",
    "dec_emb_layer = Embedding(output_vocab_size, latent_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "decoder_dense = Dense(output_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "history = model.fit([encoder_input_data, decoder_input_data],\n",
    "                    decoder_target_data[..., np.newaxis],\n",
    "                    batch_size=2,\n",
    "                    epochs=10)\n",
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "decoder_model = Model([decoder_inputs, decoder_state_input_h, decoder_state_input_c],\n",
    "                      [decoder_outputs2, state_h2, state_c2])\n",
    "\n",
    "reverse_output_index = {i: w for w, i in output_tokenizer.word_index.items()}\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = output_tokenizer.word_index['start']\n",
    "    decoded_sentence = ''\n",
    "    for _ in range(max_decoder_len):\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = reverse_output_index.get(sampled_token_index, '')\n",
    "        if sampled_word == 'end':\n",
    "            break\n",
    "        decoded_sentence += ' ' + sampled_word\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence.strip()\n",
    "\n",
    "for i in range(len(input_texts)):\n",
    "    input_seq = encoder_input_data[i:i+1]\n",
    "    print(\"Input:\", input_texts[i])\n",
    "    print(\"Predicted:\", decode_sequence(input_seq))\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8460438",
   "metadata": {},
   "source": [
    "# Part-III: Visualizing and Enhancing Encoder-Decoder\n",
    "\n",
    "## Task 8: Model Performance Discussion\n",
    "\n",
    "1. **What are the challenges in training sequence-to-sequence models?**  \n",
    "They struggle with long-term dependencies, require large datasets, and can overfit small data.\n",
    "\n",
    "2. **What does a “bad” translation look like? Why might it happen?**  \n",
    "It may omit words, repeat phrases, or produce unrelated words due to insufficient training or limited vocabulary.\n",
    "\n",
    "3. **How can the model be improved further?**  \n",
    "Use attention mechanisms, larger datasets, deeper architectures, and regularization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
