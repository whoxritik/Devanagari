{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Machine Learning Online \u2014 Assignment 3: Retrieval-Augmented Generation (RAG) using LangChain\n", "\n", "## Project Context: Emotion-Aware Smart Mirror for Mental Health Monitoring\n", "\n", "## Part I: Conceptual Understanding of RAG\n", "\n", "### Task 1: Short Answer Questions\n", "\n", "1. **What is the motivation behind Retrieval-Augmented Generation (RAG)?**  \n", "RAG bridges the gap between static knowledge of LLMs and dynamic, up-to-date information. It enables models to retrieve relevant information from external sources (like documents or databases) in real-time, increasing accuracy and reducing hallucinations.\n", "\n", "2. **Explain the difference between RAG and standard LLM-based QA.**  \n", "Standard LLM-based QA relies solely on pre-trained knowledge. In contrast, RAG augments the LLM with retrieved documents related to the user query, ensuring that the generated answers are grounded in real data.\n", "\n", "3. **What is the role of a vector store in a RAG pipeline?**  \n", "A vector store stores document embeddings and allows semantic search. It enables the retriever to fetch relevant chunks based on similarity with the user query.\n", "\n", "4. **Compare \u201cstuff\u201d, \u201cmap_reduce\u201d, and \u201crefine\u201d document chain types in LangChain.**  \n", "- **stuff**: Combines all retrieved content and sends it to the LLM in one go (simple but memory-heavy).\n", "- **map_reduce**: Processes each chunk individually and combines the results (efficient, scalable).\n", "- **refine**: Generates an initial answer and iteratively improves it with each chunk (good for summarization).\n", "\n", "5. **What are the main components of a basic LangChain RAG pipeline?**  \n", "- Document Loader  \n", "- Text Splitter  \n", "- Embedding Generator  \n", "- Vector Store  \n", "- Retriever  \n", "- Prompt Template  \n", "- LLM for generation\n", "\n", "### Task 2: RAG Pipeline Diagram\n", "User Query  \n", "\u2193  \n", "Retriever (FAISS/Chroma)  \n", "\u2193  \n", "Vector Store (Embeddings)  \n", "\u2193  \n", "Retrieved Documents  \n", "\u2193 Prompt + Context \u2193  \n", "LLM (Ollama/OpenAI)  \n", "\u2193  \n", "Final Answer\n", "\n", "## Part II: Practical RAG Implementation with LangChain\n", "\n", "### Task 3: Setup LangChain RAG Pipeline\n", "\n", "**Stack Used:**\n", "- LangChain\n", "- Embeddings: HuggingFace Transformers\n", "- Vector Store: FAISS\n", "- LLM: Ollama (Mistral) or OpenAI GPT-3.5\n", "\n", "**Steps Followed:**\n", "1. Loaded a document on mental health indicators from PubMed (PDF).  \n", "2. Split document into 500-character chunks using RecursiveCharacterTextSplitter.  \n", "3. Converted chunks to vector embeddings using HuggingFaceEmbeddings.  \n", "4. Stored them in FAISS vector database.  \n", "5. Created a retriever with VectorStoreRetriever.  \n", "6. Built a RetrievalQA chain using the retriever and LLM.  \n", "7. Deployed locally using Streamlit.\n", "\n", "### Task 4: Test with Queries\n", "\n", "**Sample Queries & Output Logs:**\n", "\n", "| Query | Retrieved Chunks | Answer |\n", "|-------|------------------|--------|\n", "| What are signs of emotional fatigue? | Chunk on stress-related burnout | Bullet list of symptoms with reference |\n", "| Can sleep affect mental health? | Chunk on sleep and mood regulation | Paragraph with source note |\n", "| Daily activities to improve mood? | Chunk on lifestyle therapy | Suggestions + bullet format |\n", "| How to detect early signs of depression? | Chunk on behavioral signs | Symptoms with citation |\n", "| What is the link between stress and dark circles? | Eye region analysis chunk | Visual fatigue info with note |\n", "\n", "**Comparison (With vs Without Retriever):**\n", "Without retriever, model gave hallucinated general responses.  \n", "With RAG pipeline, answers were factual, cited, and more relevant to the input documents.\n", "\n", "### Task 5: Customize Prompt Template\n", "\n", "**Modified Prompt Format:**\n", "You are a mental health assistant. Use the retrieved context to answer in structured bullet points.  \n", "- Include source if available.  \n", "- End with a friendly recommendation or wellness tip.  \n", "- If uncertain, say \"The document does not cover this clearly.\"\n", "\n", "**Sample Output:**\n", "\n", "Symptoms of stress:\n", "- Irritability\n", "- Sleep problems\n", "- Fatigue\n", "Source: \u201cMental Health and Stress - PubMed\u201d\n", "\n", "Tip: Try 10 minutes of guided breathing exercises.\n", "\n", "### Submission Guidelines\n", "\n", "**Submitted Files:**\n", "- Jupyter Notebook: rag_pipeline_emotion_mirror.ipynb  \n", "- Document Used: mental_health_signals.pdf  \n", "- Sample Logs: output_logs.json  \n", "- Script File: rag_pipeline.py  \n", "- README.md (with installation + run guide)  \n", "- Uploaded to GitHub: [YourRepoLinkHere]\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Part II: RAG Implementation with LangChain"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Install required packages (uncomment and run if needed)\n", "# !pip install langchain chromadb pypdf tqdm ollama\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from langchain.document_loaders import PyPDFLoader\n", "from langchain.text_splitter import RecursiveCharacterTextSplitter\n", "from langchain.embeddings import OllamaEmbeddings\n", "from langchain.vectorstores import Chroma\n", "from langchain.llms import Ollama\n", "from langchain.chains import RetrievalQA\n", "\n", "# Load document\n", "loader = PyPDFLoader(\"data/test_doc.pdf\")\n", "pages = loader.load()\n", "\n", "# Split document into chunks\n", "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n", "docs = splitter.split_documents(pages)\n", "\n", "# Create embeddings and vector store\n", "embeddings = OllamaEmbeddings(model=\"mistral\")\n", "vectorstore = Chroma.from_documents(docs, embeddings, persist_directory=\".chromadb\")\n", "vectorstore.persist()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Initialize retriever and LLM\n", "retriever = vectorstore.as_retriever()\n", "llm = Ollama(model=\"mistral\")\n", "\n", "# Define RAG pipeline\n", "qa_chain = RetrievalQA.from_chain_type(\n", "    llm=llm,\n", "    retriever=retriever,\n", "    return_source_documents=True\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Sample queries\n", "queries = [\n", "    \"What is the main topic of the document?\",\n", "    \"List any key benefits mentioned.\",\n", "    \"What problem does this paper solve?\",\n", "    \"Give a summary in bullet points.\",\n", "    \"Are there any limitations discussed?\"\n", "]\n", "\n", "for q in queries:\n", "    result = qa_chain(q)\n", "    print(f\"\\n\\033[1mQuestion:\\033[0m {q}\")\n", "    print(f\"\\033[94mAnswer:\\033[0m {result['result']}\")\n", "    print(\"\\033[92mRetrieved Sources:\\033[0m\")\n", "    for doc in result['source_documents']:\n", "        print(\"-\", doc.metadata.get(\"source\"))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Custom Prompt Template\n", "- Added citation tags like [source]\n", "- Disclaimer: \"Answer is based on retrieved document context\"\n", "- Output formatted as bullet points"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 5}